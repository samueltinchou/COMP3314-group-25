{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47681055",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- CRITICAL: Force interactive backend ---\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams['interactive'] = True\n",
    "matplotlib.rcParams['toolbar'] = 'toolmanager'\n",
    "# OR for scripts:\n",
    "# matplotlib.use('TkAgg')   # or 'Qt5Agg'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from load_data import load_raw\n",
    "from preprocess import clean_data, select_column, rename, sales_and_residential, drop_na, add_cities, make_zero, geocode, geo_address, one_hot\n",
    "from train_model import train_random_forest\n",
    "from evaluate import evaluate_model, compute_shap_local, compute_shap_global, feature_i, shap_initialise, compute_shap_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ee2275",
   "metadata": {},
   "source": [
    "Step 1: Load data and delete the data that is not our target cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9648718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = r\"C:\\Users\\ktlee\\Downloads\\all_transactions.csv\"\n",
    "df_raw = load_raw(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f89b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_renamed = rename(df_raw)\n",
    "df_selected = select_column(df_renamed)\n",
    "df_filtered = sales_and_residential(df_selected)\n",
    "df_dropped = drop_na(df_filtered)\n",
    "df_with_city = add_cities(df_dropped)\n",
    "df_with_zero = make_zero(df_with_city)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414b6934",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Externally download the street data\n",
    "df_with_zero['Full addr'] = (\n",
    "        df_with_zero['Street number'].astype(str).str.strip() + ' ' +\n",
    "        df_with_zero['Street name'].astype(str).str.strip() + ', ' +\n",
    "        df_with_zero['Postal code'].astype(str).str.strip() + ' ' +\n",
    "        df_with_zero['Municipality'].astype(str).str.strip()\n",
    "    )\n",
    "df = df_with_zero.copy()\n",
    "df.drop(columns=['Street number', 'Street name', 'Postal code', 'Municipality'], inplace=True)\n",
    "#df.to_csv(r\"C:\\Users\\ktlee\\Downloads\\clean_data_new.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644d56e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add back the longtitude and latitude\n",
    "df_address = pd.read_csv(r\"C:\\Users\\ktlee\\Downloads\\clean_address_dict.csv\")\n",
    "df_address.columns = ['Full addr', 'Longitude', 'Latitude']\n",
    "#df_address.head\n",
    "#df_with_zero.columns\n",
    "df_geocoded = df_with_zero.merge(df_address, on = 'Full addr', how = 'left')\n",
    "\n",
    "df_clean_geo = df_geocoded[\n",
    "    (df_geocoded['Longitude'].notna() & df_geocoded['Latitude'].notna()) &\n",
    "    (df_geocoded['Longitude'] != 0) & (df_geocoded['Latitude'] != 0)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6f2687",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot coding\n",
    "df_all_eng = one_hot(df_clean_geo)\n",
    "df_all_eng.dtypes\n",
    "\n",
    "df_all_eng.to_csv(r\"C:\\Users\\ktlee\\Downloads\\df_all_eng.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3e97f8",
   "metadata": {},
   "source": [
    "Export/Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33332eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# Create cache directory\n",
    "CACHE_DIR = Path(\"model_cache\")\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def save_city_artifacts(city: str, artifacts: dict):\n",
    "    \"\"\"\n",
    "    Save all model artifacts for a city to disk.\n",
    "    \"\"\"\n",
    "    path = CACHE_DIR / f\"{city}.joblib\"\n",
    "    joblib.dump(artifacts, path)\n",
    "    print(f\"Exported artifacts for {city} â†’ {path}\")\n",
    "\n",
    "def load_city_artifacts(city: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load previously saved artifacts.\n",
    "    \"\"\"\n",
    "    path = CACHE_DIR / f\"{city}.joblib\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"No cached model for {city}\")\n",
    "    artifacts = joblib.load(path)\n",
    "    print(f\"Loaded cached artifacts for {city}\")\n",
    "    return artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2de42ba",
   "metadata": {},
   "source": [
    "Step 2: For each city, pre-process and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fd33e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_all_eng = pd.read_csv('../data/df_all_eng.csv')\n",
    "df_all_eng['City'] = np.where(df_all_eng['City'] == 'Montepliier', 'Montepiller', df_all_eng['City'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d42381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all_eng\n",
    "cities = df['City'].unique()\n",
    "\n",
    "#print(cities)\n",
    "\n",
    "# For now, use sample for testing\n",
    "#df_sample = df.head(1000).copy()  # Remove later\n",
    "#df_sample = geocode(df_sample)   # Test geocoding\n",
    "\n",
    "metrics_table = dict()\n",
    "feature_table = dict()\n",
    "\n",
    "training_city = ['Montepiller']\n",
    "\n",
    "for city in cities:\n",
    "    if city not in training_city:\n",
    "        continue\n",
    "    df_city = df[df['City'] == city].copy()\n",
    "    #print(df_city['Property value'])\n",
    "    X, y = clean_data(df_city)\n",
    "\n",
    "    case_model = 1 #<-- Change this\n",
    "\n",
    "    #Case 3: Only print output, without machine learning\n",
    "    if case_model == 3:\n",
    "        print(f\"{city}: {df_city.describe()}\")\n",
    "\n",
    "    #Case 1: Let the program to find best model\n",
    "    if case_model == 1:\n",
    "        model, params, X_test, y_test, best_params = train_random_forest(X, y)\n",
    "\n",
    "    #Case 2: Self define the model\n",
    "    if case_model == 2:\n",
    "        param_grid = {\n",
    "                'n_estimators': [2500],\n",
    "                'max_depth': [32],\n",
    "                'bootstrap': [False]\n",
    "            }\n",
    "        model, params, X_test, y_test = train_random_forest(X, y, param_grid)\n",
    "        print(\"Model training finished\")\n",
    "\n",
    "    if case_model == 4:\n",
    "        artifacts = {\n",
    "            'city' : training_city,\n",
    "            'X': X, 'y': y, 'model': model,\n",
    "            'X_test': X_test, 'y_test': y_test,\n",
    "            'best_params': best_params\n",
    "        }\n",
    "        save_city_artifacts(city, artifacts)\n",
    "    \n",
    "    # Evaluate\n",
    "    if case_model in [1,2]:\n",
    "        metrics = evaluate_model(model, X_test, y_test)\n",
    "        metrics_table[city] = metrics\n",
    "        feature_importance = feature_i(model, X_test)\n",
    "        print(feature_importance)\n",
    "        feature_table[city] = feature_importance\n",
    "\n",
    "\n",
    "        explain, shap_values, X_sample = shap_initialise(model, X_test, 100)\n",
    "\n",
    "        #compute_shap_local(model, X_test, n = 100)\n",
    "        compute_shap_global(shap_values, X_sample)\n",
    "        #compute_shap_cluster(explain, shap_values, X_sample)\n",
    "        \n",
    "        print(f\"City {city}'s model training is done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e243833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def update_merged_summary(city, metrics, feature_importance, base_path=\"../reports\"):\n",
    "    # Prepare row for this city\n",
    "    row = {'City': city}\n",
    "    row.update(metrics)\n",
    "\n",
    "    for feature_name, importance in feature_importance.items():\n",
    "        row[f'Feature: {feature_name}'] = importance\n",
    "        \n",
    "    new_row_df = pd.DataFrame([row]).set_index('City')\n",
    "\n",
    "    # Path to the summary file\n",
    "    summary_path = os.path.join(base_path, \"merged_summary.csv\")\n",
    "\n",
    "    # Load existing or create new\n",
    "    if os.path.exists(summary_path):\n",
    "        merged_df = pd.read_csv(summary_path, index_col='City')\n",
    "        merged_df = pd.concat([merged_df, new_row_df])\n",
    "    else:\n",
    "        merged_df = new_row_df\n",
    "\n",
    "    # Save updated summary\n",
    "    merged_df.to_csv(summary_path)\n",
    "    print(f\"Updated summary saved to {summary_path}\")\n",
    "\n",
    "for city in metrics_table:\n",
    "    update_merged_summary(\n",
    "        city=city,\n",
    "        metrics=metrics_table[city],\n",
    "        feature_importance=feature_table[city]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95208c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'max_depth': 16, 'n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "print(best_params)\n",
    "with open('../reports/best_params.txt', 'a') as f:\n",
    "    f.write(f\"'City': {best_params}\")\n",
    "f.close()\n",
    "#compute_shap_global(shap_values, X_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
