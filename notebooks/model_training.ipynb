{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47681055",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- CRITICAL: Force interactive backend ---\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams['interactive'] = True\n",
    "matplotlib.rcParams['toolbar'] = 'toolmanager'\n",
    "# OR for scripts:\n",
    "# matplotlib.use('TkAgg')   # or 'Qt5Agg'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from load_data import load_raw\n",
    "from preprocess import clean_data, select_column, rename, sales_and_residential, drop_na, add_cities, make_zero, geocode, geo_address, one_hot\n",
    "from train_model import train_random_forest\n",
    "from evaluate import evaluate_model, compute_shap_local, compute_shap_global, feature_i, shap_initialise, compute_shap_cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ee2275",
   "metadata": {},
   "source": [
    "Step 1: Load data and delete the data that is not our target cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9648718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = r\"C:\\Users\\ktlee\\Downloads\\all_transactions.csv\"\n",
    "df_raw = load_raw(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f89b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_renamed = rename(df_raw)\n",
    "df_selected = select_column(df_renamed)\n",
    "df_filtered = sales_and_residential(df_selected)\n",
    "df_dropped = drop_na(df_filtered)\n",
    "df_with_city = add_cities(df_dropped)\n",
    "df_with_zero = make_zero(df_with_city)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414b6934",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Externally download the street data\n",
    "df_with_zero['Full addr'] = (\n",
    "        df_with_zero['Street number'].astype(str).str.strip() + ' ' +\n",
    "        df_with_zero['Street name'].astype(str).str.strip() + ', ' +\n",
    "        df_with_zero['Postal code'].astype(str).str.strip() + ' ' +\n",
    "        df_with_zero['Municipality'].astype(str).str.strip()\n",
    "    )\n",
    "df = df_with_zero.copy()\n",
    "df.drop(columns=['Street number', 'Street name', 'Postal code', 'Municipality'], inplace=True)\n",
    "#df.to_csv(r\"C:\\Users\\ktlee\\Downloads\\clean_data_new.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644d56e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add back the longtitude and latitude\n",
    "df_address = pd.read_csv(r\"C:\\Users\\ktlee\\Downloads\\clean_address_dict.csv\")\n",
    "df_address.columns = ['Full addr', 'Longitude', 'Latitude']\n",
    "#df_address.head\n",
    "#df_with_zero.columns\n",
    "df_geocoded = df_with_zero.merge(df_address, on = 'Full addr', how = 'left')\n",
    "\n",
    "df_clean_geo = df_geocoded[\n",
    "    (df_geocoded['Longitude'].notna() & df_geocoded['Latitude'].notna()) &\n",
    "    (df_geocoded['Longitude'] != 0) & (df_geocoded['Latitude'] != 0)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6f2687",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot coding\n",
    "df_all_eng = one_hot(df_clean_geo)\n",
    "df_all_eng.dtypes\n",
    "\n",
    "df_all_eng.to_csv(r\"C:\\Users\\ktlee\\Downloads\\df_all_eng.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2de42ba",
   "metadata": {},
   "source": [
    "Step 2: For each city, pre-process and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fd33e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_all_eng = pd.read_csv('../data/df_all_eng.csv')\n",
    "df_all_eng['City'] = np.where(df_all_eng['City'] == 'Montepliier', 'Montepiller', df_all_eng['City'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d64a211",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all_eng\n",
    "cities = df['City'].unique()\n",
    "\n",
    "#print(cities)\n",
    "\n",
    "# For now, use sample for testing\n",
    "#df_sample = df.head(1000).copy()  # Remove later\n",
    "#df_sample = geocode(df_sample)   # Test geocoding\n",
    "\n",
    "metrics_table = dict()\n",
    "feature_table = dict()\n",
    "\n",
    "training_city = ['Montepiller', 'Nantes']\n",
    "\n",
    "for city in cities:\n",
    "    if city not in training_city:\n",
    "        continue\n",
    "    df_city = df[df['City'] == city].copy()\n",
    "    #print(df_city['Property value'])\n",
    "    X, y = clean_data(df_city)\n",
    "\n",
    "    case_model = 1 #<-- Change this\n",
    "\n",
    "    #Case 3: Only print output, without machine learning\n",
    "    if case_model == 3:\n",
    "        print(f\"{city}: {df_city.describe()}\")\n",
    "\n",
    "    #Case 1: Let the program to find best model\n",
    "    if case_model == 1:\n",
    "        model, params, X_test, y_test, best_params = train_random_forest(X, y)\n",
    "\n",
    "    #Case 2: Self define the model\n",
    "    if case_model == 2:\n",
    "        param_grid = {\n",
    "                'n_estimators': [2500],\n",
    "                'max_depth': [32],\n",
    "                'bootstrap': [False]\n",
    "            }\n",
    "        model, params, X_test, y_test = train_random_forest(X, y, param_grid)\n",
    "        print(\"Model training finished\")\n",
    "\n",
    "    # Evaluate\n",
    "    if case_model in [1,2]:\n",
    "        metrics = evaluate_model(model, X_test, y_test)\n",
    "        metrics_table[city] = metrics\n",
    "        feature_importance = feature_i(model, X_test)\n",
    "        print(feature_importance)\n",
    "        feature_table[city] = feature_importance\n",
    "\n",
    "\n",
    "        explain, shap_values, X_sample = shap_initialise(model, X_test, 100)\n",
    "\n",
    "        #compute_shap_local(model, X_test, n = 100)\n",
    "        compute_shap_global(explain, shap_values, X_sample)\n",
    "        #compute_shap_cluster(explain, shap_values, X_sample)\n",
    "        \n",
    "        explain_table = dict()\n",
    "        shap_values_table = dict()\n",
    "        X_sample_table = dict()\n",
    "\n",
    "        explain_table[city] = explain\n",
    "        shap_values_table[city] = shap_values\n",
    "        X_sample_table[city] = \n",
    "        \n",
    "        print(f\"City {city}'s model training is done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f11f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain, shap_values, X_sample = shap_initialise(model, X_test, 100)\n",
    "\n",
    "print(\"shap_values type :\", type(shap_values))\n",
    "print(\"shap_values shape:\", shap_values.shape if hasattr(shap_values, 'shape') else [a.shape for a in shap_values])\n",
    "\n",
    "if True:\n",
    "    #compute_shap_local(model, X_test, n = 100)\n",
    "    compute_shap_global(explain, shap_values, X_sample)\n",
    "    compute_shap_cluster(explain, shap_values, X_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
